{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DGO0EMfpcmY1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZZDdVnMayrs"
      },
      "source": [
        "#Loading Mnist dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na87Tu3Ddt9S",
        "outputId": "93b9630b-8160-4358-f513-1e12a3970324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9D7-T6fdzdy",
        "outputId": "d04e0dc4-e58d-4f9a-e6e6-b5b546f06eca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 28, 28) (10000, 28, 28) \n",
            "\n",
            "(60000,) (10000,) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#x_train = x_train[0:10000,:]\n",
        "#x_test = x_test[0:1000,:]\n",
        "print(x_train.shape,x_test.shape,\"\\n\")\n",
        "#y_train = y_train[0:10000]\n",
        "#y_test = y_test[0:1000]\n",
        "print(y_train.shape,y_test.shape,\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja-dmBFKXe0O"
      },
      "source": [
        "#Doing dimensionality reduction for Training set\n",
        "#and get the centroid of each sub-image, after splitting the image into 4X4 sub-images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JOpdEC6Fd250"
      },
      "outputs": [],
      "source": [
        "k = 0\n",
        "j=0\n",
        "x_axis = 0\n",
        "y_axis = 0\n",
        "array_train = np.zeros([60000,32])\n",
        "for t in range(0,x_train.shape[0],1):\n",
        "\n",
        "    x_axis = 0\n",
        "    y_axis = 0\n",
        "    j = 0\n",
        "    k = 7\n",
        "    h = 7\n",
        "    i = 0\n",
        "    l = 0\n",
        "    img = np.zeros([16,7,7])\n",
        "\n",
        "    for i in range(0,16):\n",
        "        if i%4 == 0 and i != 0:\n",
        "            k += 7\n",
        "            h = 7\n",
        "        img[i] = x_train[t][k-7:k,h-7:h]\n",
        "        h += 7\n",
        "\n",
        "    #img[0] = x_train[t][0:9,0:9]\n",
        "    #img[1] = x_train[t][0:9,9:18]\n",
        "    #img[2] = x_train[t][0:9,18:27]\n",
        "    #img[3] = x_train[t][9:18,0:9]\n",
        "    #img[4] = x_train[t][9:18,9:18]\n",
        "    #img[5] = x_train[t][9:18,18:27]\n",
        "    #img[6] = x_train[t][18:27,0:9]\n",
        "    #img[7] = x_train[t][18:27,9:18]\n",
        "    #img[8] = x_train[t][18:27,18:27]\n",
        "                                        #00000000\n",
        "    for l in range(0,16):\n",
        "        for i in range(0,7):\n",
        "            for j in range(0,7):\n",
        "                x_axis += img[l][i][j]*i\n",
        "\n",
        "        if sum(sum(img[l])) == 0:\n",
        "            array_train[t][l] = 0\n",
        "\n",
        "        else:\n",
        "            array_train[t][l] = x_axis/sum(sum(img[l]))\n",
        "        x_axis = 0\n",
        "\n",
        "    ################################################\n",
        "\n",
        "    i = 0\n",
        "    j = 0\n",
        "    l = 0\n",
        "    y_axis = 0\n",
        "    for l in range(0,16):\n",
        "        for i in range(0,7):\n",
        "            for j in range(0,7):\n",
        "                y_axis = y_axis + img[l][j][i]*i\n",
        "\n",
        "        if sum(sum(img[l])) == 0:\n",
        "            array_train[t][l+16] = 0\n",
        "        else:\n",
        "            array_train[t][l+16] = y_axis/sum(sum(img[l]))\n",
        "        y_axis = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqVJHeo3bE_r"
      },
      "source": [
        "#Doing dimensionality reduction for Testing set\n",
        "#and get the centroid of each sub-image, after splitting the image into 4X4 sub-images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MTS5M0WTd4-b"
      },
      "outputs": [],
      "source": [
        "k = 0\n",
        "j=0\n",
        "x_axis = 0\n",
        "y_axis = 0\n",
        "array_test = np.zeros([10000,32])\n",
        "for t in range(0,x_test.shape[0],1):\n",
        "\n",
        "    x_axis = 0\n",
        "    y_axis = 0\n",
        "    j = 0\n",
        "\n",
        "    i = 0\n",
        "    l = 0\n",
        "    img = np.zeros([16,7,7])\n",
        "\n",
        "    k = 7\n",
        "    h = 7\n",
        "    for i in range(0,16):\n",
        "        if i%4 == 0 and i != 0:\n",
        "            k += 7\n",
        "            h = 7\n",
        "        img[i] = x_test[t][k-7:k,h-7:h]\n",
        "        h += 7\n",
        "\n",
        "    #img[0] = x_test[t][0:9,0:9]\n",
        "    #img[1] = x_test[t][0:9,9:18]\n",
        "    #img[2] = x_test[t][0:9,18:27]\n",
        "    #img[3] = x_test[t][9:18,0:9]\n",
        "    #img[4] = x_test[t][9:18,9:18]\n",
        "    #img[5] = x_test[t][9:18,18:27]\n",
        "    #img[6] = x_test[t][18:27,0:9]\n",
        "    #img[7] = x_test[t][18:27,9:18]\n",
        "    #img[8] = x_test[t][18:27,18:27]\n",
        "\n",
        "    for l in range(0,16):\n",
        "        for i in range(0,7):\n",
        "            for j in range(0,7):\n",
        "                x_axis += img[l][i][j]*i\n",
        "\n",
        "        if sum(sum(img[l])) == 0:\n",
        "            array_test[t][l] = 0\n",
        "\n",
        "        else:\n",
        "            array_test[t][l] = x_axis/sum(sum(img[l]))\n",
        "        x_axis = 0\n",
        "    ################################################\n",
        "\n",
        "    i = 0\n",
        "    j = 0\n",
        "    l = 0\n",
        "    y_axis = 0\n",
        "\n",
        "    for l in range(0,16,1):\n",
        "        for i in range(0,7):\n",
        "            for j in range(0,7):\n",
        "                y_axis = y_axis + img[l][j][i]*i\n",
        "\n",
        "        if sum(sum(img[l])) == 0:\n",
        "            array_test[t][l+16] = 0\n",
        "        else:\n",
        "            array_test[t][l+16] = y_axis/sum(sum(img[l]))\n",
        "        y_axis = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--kJBNC2gcaL"
      },
      "source": [
        "#Normalize the features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TOUhV4dEgbv_"
      },
      "outputs": [],
      "source": [
        "for i in range (array_train.shape[1]):\n",
        "    array_train[:,i] = (array_train[:,i] - np.mean(array_train[:,i])) / np.std(array_train[:,i])\n",
        "    array_test[:,i] = (array_test[:,i] - np.mean(array_test[:,i])) / np.std(array_test[:,i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjKKqzvPUtBJ"
      },
      "source": [
        "#Creating validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ikl3i1MXUXS6"
      },
      "outputs": [],
      "source": [
        "array_val = array_train[:6000,:]\n",
        "array_train = array_train[6000:,:]\n",
        "y_val = y_train[:6000]\n",
        "y_train = y_train[6000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tiWm1suU21z",
        "outputId": "c1a3a01f-8525-4675-a797-040d88907f4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((6000, 32), (6000,), (54000, 32), (54000,))"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "array_val.shape, y_val.shape, array_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYWdeDbrbku6"
      },
      "source": [
        "#implementing sigmoid activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4eTXyu1AhyIH"
      },
      "outputs": [],
      "source": [
        "def sigmoid(array):\n",
        "    #h = array.copy()\n",
        "    #h = -h\n",
        "    #h = np.exp(h)\n",
        "    #h = h + 1\n",
        "    #h = 1 / h\n",
        "    return 1/(1+np.exp(-array.copy(),dtype = np.float128))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGlqgHSffU1i"
      },
      "source": [
        "#Defining Softmax activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "B2gfekGzXpfs"
      },
      "outputs": [],
      "source": [
        "def softmax(array):\n",
        "    post_values = array.copy()\n",
        "    post_values = np.exp(post_values, dtype = np.float128)\n",
        "    sum_of_output_values = np.sum(post_values.copy(),axis=1)\n",
        "    #sum_of_output_values = np.exp(sum_of_output_values, dtype = np.float128)\n",
        "    for i in range(post_values.shape[1]):\n",
        "        post_values[:,i] /= sum_of_output_values\n",
        "    return post_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dwh9AhCYfJGh"
      },
      "source": [
        "#Defining function convert the integer number to a one-hot dimension with respect to the number of classes in the set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jQSXxp5Nh3oF"
      },
      "outputs": [],
      "source": [
        "def dimensions(y_train):\n",
        "    real_y = np.zeros(shape=(1,10))\n",
        "\n",
        "    real_y[:,y_train] = 1\n",
        "    return real_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McdTuWxne7nk"
      },
      "source": [
        "#Defining function computes the gradient **rate of change** of the loss values with respect to the weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hYpWvACih8N-"
      },
      "outputs": [],
      "source": [
        "def sigmoid_gradient(array):\n",
        "    return np.multiply(array,1-array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjHE4BH4ezxg"
      },
      "source": [
        "#Defining the loss function **Cross entropy** for classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "bZs8mC8eKtWA"
      },
      "outputs": [],
      "source": [
        "def loss_function(x,y,parameters, reg_lambda):\n",
        "  h_theta = x\n",
        "  real_y = dimensions_10(y)\n",
        "  m = h_theta.shape[0]\n",
        "\n",
        "  for i in range(parameters.shape[0]-1):\n",
        "    h_theta = sigmoid(np.dot(h_theta,parameters[i].parameter_matrix))\n",
        "    #h_theta = sigmoid(h_theta)\n",
        "\n",
        "  h_theta = softmax(np.dot(h_theta,parameters[parameters.shape[0]-1].parameter_matrix))\n",
        "  #h_theta = softmax(h_theta)\n",
        "  cost = np.multiply(real_y.copy(),np.log(h_theta)) + np.multiply(1-real_y.copy(),np.log(1-h_theta))\n",
        "  #regularized_cost = np.sum((np.square(np.array(parameters)))) * (reg_lambda/(2*m))\n",
        "  regularized_cost = 0\n",
        "  for i in range(parameters.shape[0]):\n",
        "    regularized_cost += np.sum((np.square(np.array(parameters[i].parameter_matrix))))\n",
        "  regularized_cost *= (reg_lambda/(2*m))\n",
        "  cost = np.sum(cost)\n",
        "  cost /= m\n",
        "  cost = -cost\n",
        "  cost += regularized_cost\n",
        "  print(\"cost is \", end=\"\")\n",
        "  return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZUPUbYjedKA"
      },
      "source": [
        "#Defining the matrix class which is desired for initalize the weight matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SrB3ijS2h-YQ"
      },
      "outputs": [],
      "source": [
        "class matrix:\n",
        "    def __init__(self, current_layer, next_layer):\n",
        "        self.current_layer = current_layer\n",
        "        self.next_layer = next_layer\n",
        "        self.parameter_matrix = np.random.normal(size=(self.current_layer,self.next_layer))\n",
        "\n",
        "    def get_parameters(self):\n",
        "        return self.parameter_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUfUveereOm6"
      },
      "source": [
        "#Defining the Neural Network class to build the desired architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yFong6SpiASa"
      },
      "outputs": [],
      "source": [
        "class Neural_Network:\n",
        "    def __init__(self, layers, neurons):\n",
        "        self.layers = layers\n",
        "        self.neurons = neurons\n",
        "        self.parameter_matrices = np.empty(shape=(self.layers - 1,),dtype=matrix)\n",
        "        self.outputs = np.zeros(shape=(self.layers,),dtype=matrix)\n",
        "\n",
        "\n",
        "\n",
        "    def preparing(self, train):\n",
        "\n",
        "        data = train.copy()\n",
        "        self.parameter_matrices[0] = matrix(data.shape[1], self.neurons)\n",
        "        self.parameter_matrices[self.layers - 2] = matrix(self.neurons, 10)\n",
        "\n",
        "        for i in range(1,self.layers - 2):\n",
        "            self.parameter_matrices[i] = matrix(self.neurons, self.neurons)\n",
        "\n",
        "        return self.parameter_matrices\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def feed_forward(self,train, parameters):\n",
        "        data = train.copy()\n",
        "        self.outputs[0] = matrix(data.shape[0], data.shape[1])\n",
        "        self.outputs[0].parameter_matrix = data.copy()\n",
        "\n",
        "        for i in range(1,self.layers-1):\n",
        "\n",
        "            data = sigmoid(np.dot(data,parameters[i-1].parameter_matrix))\n",
        "\n",
        "            self.outputs[i] = matrix(data.shape[0], data.shape[1])\n",
        "            self.outputs[i].parameter_matrix = data.copy()\n",
        "\n",
        "\n",
        "        data = softmax(np.dot(data,parameters[self.layers - 2].parameter_matrix))\n",
        "\n",
        "\n",
        "        self.outputs[self.layers - 1] = matrix(data.shape[0], data.shape[1])\n",
        "        self.outputs[self.layers - 1].parameter_matrix = data.copy()\n",
        "\n",
        "\n",
        "        return parameters.copy(), self.outputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def back_propagation(self, outputs, parameters, target, learning_rate, reg_lambda):\n",
        "        actual_y = dimensions(target)\n",
        "\n",
        "        thetas = parameters.copy()\n",
        "        parameter = parameters.copy()\n",
        "        data = outputs.copy()\n",
        "\n",
        "        ########################################################\n",
        "        # for output layer only\n",
        "        error = np.multiply((actual_y - data[-1].parameter_matrix), sigmoid_gradient(data[-1].parameter_matrix))\n",
        "\n",
        "        thetas[-1].parameter_matrix += np.dot(np.transpose(data[-2].parameter_matrix), error) * learning_rate\n",
        "        # for output layer only\n",
        "        #########################################################\n",
        "\n",
        "        #########################################################\n",
        "        # for hidden layers...\n",
        "\n",
        "        for i in range(data.shape[0] - 2, 0 , -1):\n",
        "              error = np.dot(error,np.transpose(parameter[i].parameter_matrix))\n",
        "              error = np.multiply(error, sigmoid_gradient(data[i].parameter_matrix))\n",
        "              thetas[i-1].parameter_matrix += (np.dot(np.transpose(data[i-1].parameter_matrix), error) + (reg_lambda/data.shape[0])*data[i-1].parameter_matrix) * learning_rate\n",
        "\n",
        "\n",
        "        return thetas.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJOwCWUTeBYK"
      },
      "source": [
        "#Defining a function convert the target to on-hot dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Yhglkh-CI5ZZ"
      },
      "outputs": [],
      "source": [
        "def dimensions_10(y_train):\n",
        "    real_y = np.zeros(shape=(y_train.shape[0],np.unique(y_train).shape[0]))\n",
        "\n",
        "    for i in range(0,y_train.shape[0]):\n",
        "        real_y[i,y_train[i]] = 1\n",
        "    return real_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7vBH1dld9QP"
      },
      "source": [
        "#Defining the accuracy test function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MWXLqbWyn1Td"
      },
      "outputs": [],
      "source": [
        "def accuracy(data,target,parameters):\n",
        "    real_y = dimensions_10(target.copy())\n",
        "    results = data.copy()\n",
        "    thetas = parameters.copy()\n",
        "\n",
        "    for i in range(thetas.shape[0]-1):\n",
        "        results = sigmoid(np.dot(results, thetas[i].parameter_matrix))\n",
        "\n",
        "    results = softmax(np.dot(results, thetas[thetas.shape[0]-1].parameter_matrix))\n",
        "\n",
        "    hypothesis = np.zeros([results.shape[0],results.shape[1]])\n",
        "    count = 0\n",
        "\n",
        "    for m in range(0,results.shape[0]):\n",
        "        index_ = 0\n",
        "        biggest = results[m,index_]\n",
        "\n",
        "        for n in range(1,results.shape[1]):\n",
        "            if biggest < results[m,n]:\n",
        "                biggest = results[m,n]\n",
        "                index_ = n\n",
        "\n",
        "        hypothesis[m,index_] = 1\n",
        "\n",
        "\n",
        "    for l in range(0,real_y.shape[0]):\n",
        "        for v in range(0,real_y.shape[1]):\n",
        "            if real_y[l,v] == 1 and hypothesis[l,v] == 1:\n",
        "                count = count + 1\n",
        "\n",
        "\n",
        "    return(count/results.shape[0])*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cVEkBQ4dyrq"
      },
      "source": [
        "#initalize the Neural Network architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "d6-ao4Mgudzz"
      },
      "outputs": [],
      "source": [
        "N = Neural_Network(7,32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynPKqVE0dstv"
      },
      "source": [
        "#Defining fit Function for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "-64wPwx7lSHC"
      },
      "outputs": [],
      "source": [
        "def fit(array_train, iterations, learning_rate, reg_lambda):\n",
        "    thetas = N.preparing(array_train.copy())\n",
        "    for iterations in range(0,iterations):\n",
        "        for i in range(0,array_train.shape[0]):\n",
        "            thetas, outputs = N.feed_forward(array_train[i:i+1,:],thetas)\n",
        "            thetas = N.back_propagation(outputs,thetas,y_train[i],learning_rate,reg_lambda)\n",
        "\n",
        "        print(\"epoch \",iterations+1,\": \\nLoss function on Training set\",loss_function(array_train,y_train,thetas, reg_lambda))\n",
        "        print(\"epoch \",iterations+1,\": \\nLoss function on Validation set\",loss_function(array_val,y_val,thetas, reg_lambda))\n",
        "        print(\"=========================================================\")\n",
        "    return thetas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAqGhPGn0jHZ",
        "outputId": "7d074cc8-d635-4824-d19f-da059505bf2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cost is epoch  1 : \n",
            "Loss function on Training set 1.9423996217288361535\n",
            "cost is epoch  1 : \n",
            "Loss function on Validation set 1.9203822946333223181\n",
            "=========================================================\n",
            "cost is epoch  2 : \n",
            "Loss function on Training set 1.581446413503816151\n",
            "cost is epoch  2 : \n",
            "Loss function on Validation set 1.5457556310622023123\n",
            "=========================================================\n",
            "cost is epoch  3 : \n",
            "Loss function on Training set 1.3654801369579258943\n",
            "cost is epoch  3 : \n",
            "Loss function on Validation set 1.3236890452584929444\n",
            "=========================================================\n",
            "cost is epoch  4 : \n",
            "Loss function on Training set 1.1961149764961481247\n",
            "cost is epoch  4 : \n",
            "Loss function on Validation set 1.1605574923461992306\n",
            "=========================================================\n",
            "cost is epoch  5 : \n",
            "Loss function on Training set 1.0730982596182314584\n",
            "cost is epoch  5 : \n",
            "Loss function on Validation set 1.0466004254966184612\n",
            "=========================================================\n",
            "cost is epoch  6 : \n",
            "Loss function on Training set 0.9865786270821253579\n",
            "cost is epoch  6 : \n",
            "Loss function on Validation set 0.9674789162551152936\n",
            "=========================================================\n",
            "cost is epoch  7 : \n",
            "Loss function on Training set 0.92153751555340929474\n",
            "cost is epoch  7 : \n",
            "Loss function on Validation set 0.9106069347949879933\n",
            "=========================================================\n",
            "cost is epoch  8 : \n",
            "Loss function on Training set 0.87517302867751592334\n",
            "cost is epoch  8 : \n",
            "Loss function on Validation set 0.87137729620537071563\n",
            "=========================================================\n",
            "cost is epoch  9 : \n",
            "Loss function on Training set 0.8401347133448531891\n",
            "cost is epoch  9 : \n",
            "Loss function on Validation set 0.84246652681745707366\n",
            "=========================================================\n",
            "cost is epoch  10 : \n",
            "Loss function on Training set 0.81072558883739736943\n",
            "cost is epoch  10 : \n",
            "Loss function on Validation set 0.81913569109535270353\n",
            "=========================================================\n",
            "cost is epoch  11 : \n",
            "Loss function on Training set 0.7845296950888217464\n",
            "cost is epoch  11 : \n",
            "Loss function on Validation set 0.7989222336800394417\n",
            "=========================================================\n",
            "cost is epoch  12 : \n",
            "Loss function on Training set 0.7616915080053076634\n",
            "cost is epoch  12 : \n",
            "Loss function on Validation set 0.78065997456243062843\n",
            "=========================================================\n",
            "cost is epoch  13 : \n",
            "Loss function on Training set 0.7420518880435203603\n",
            "cost is epoch  13 : \n",
            "Loss function on Validation set 0.7647291181808406306\n",
            "=========================================================\n",
            "cost is epoch  14 : \n",
            "Loss function on Training set 0.72611048969327742186\n",
            "cost is epoch  14 : \n",
            "Loss function on Validation set 0.7519172076710110378\n",
            "=========================================================\n",
            "cost is epoch  15 : \n",
            "Loss function on Training set 0.71339474103723288614\n",
            "cost is epoch  15 : \n",
            "Loss function on Validation set 0.74108188440026883867\n",
            "=========================================================\n",
            "cost is epoch  16 : \n",
            "Loss function on Training set 0.7018392887920522161\n",
            "cost is epoch  16 : \n",
            "Loss function on Validation set 0.7304903480526563088\n",
            "=========================================================\n",
            "cost is epoch  17 : \n",
            "Loss function on Training set 0.68993882394372481\n",
            "cost is epoch  17 : \n",
            "Loss function on Validation set 0.71954847059949814564\n",
            "=========================================================\n",
            "cost is epoch  18 : \n",
            "Loss function on Training set 0.67881423565720669193\n",
            "cost is epoch  18 : \n",
            "Loss function on Validation set 0.70913579018294539284\n",
            "=========================================================\n",
            "cost is epoch  19 : \n",
            "Loss function on Training set 0.66969707314739549255\n",
            "cost is epoch  19 : \n",
            "Loss function on Validation set 0.70019016347211035\n",
            "=========================================================\n",
            "cost is epoch  20 : \n",
            "Loss function on Training set 0.6615433265331242729\n",
            "cost is epoch  20 : \n",
            "Loss function on Validation set 0.6923882912027692834\n",
            "=========================================================\n"
          ]
        }
      ],
      "source": [
        "#for iterations in range(0,50):\n",
        "#    for i in range(0,array_train.shape[0]):\n",
        "#        thetas, outputs = N.feed_forward(array_train[i:i+1,:],thetas)\n",
        "#        thetas = N.back_propagation(outputs,thetas,y_train[i],0.01)\n",
        "\n",
        " #   print(iterations+1,\": \",loss_function(a/rray_train,y_train,thetas))\n",
        "thetas = fit(array_train, 20, 0.01, 1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1fMXe3rfvx2"
      },
      "source": [
        "#printing the accuracy after training on the training set on **Training set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvNG-fhwoIqA",
        "outputId": "6dbffbec-e9c8-4486-b649-bd2cdd553def"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy of training set is:  88.27222222222221\n"
          ]
        }
      ],
      "source": [
        "print(\"accuracy of training set is: \", accuracy(array_train,y_train,thetas))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7OTIb_1qGx_"
      },
      "source": [
        "#printing the accuracy after training on the training set on **Test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNL7PBbWJc7U",
        "outputId": "b41a2e6f-e5eb-4c2c-b4f5-00a6c9ff174d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy on test set:  88.08\n"
          ]
        }
      ],
      "source": [
        "print(\"accuracy on test set: \", accuracy(array_test,y_test,thetas))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-CSIEgFkcUO"
      },
      "source": [
        "#defining a function preparing for prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "_F-obhLokbRW"
      },
      "outputs": [],
      "source": [
        "def preparing(result,parameters):\n",
        "    results = result.copy()\n",
        "    thetas = parameters.copy()\n",
        "    for i in range(thetas.shape[0]-1):\n",
        "        results = sigmoid(np.dot(results, thetas[i].parameter_matrix))\n",
        "\n",
        "    results = softmax(np.dot(results, thetas[thetas.shape[0]-1].parameter_matrix))\n",
        "\n",
        "    post_result = np.zeros(shape=(results.shape[0],))\n",
        "    for i in range(results.shape[0]):\n",
        "        index = np.argmax(results[i,:])\n",
        "        post_result[i]=index\n",
        "    return post_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXd1ce02gQGF"
      },
      "source": [
        "#See the Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "zOg6VPPBgQQC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Qt-NcLUeiP9A"
      },
      "outputs": [],
      "source": [
        "results = preparing(array_test,thetas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v6H4071uRf0"
      },
      "source": [
        "#Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy_YvN4rhznV",
        "outputId": "261b4268-0e2e-4252-9a0d-3847531c29f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 958,    0,    1,    2,    2,    6,    7,    1,    2,    1],\n",
              "       [   2, 1083,   18,    8,    4,    1,    5,    5,    6,    3],\n",
              "       [  18,    7,  901,   47,    4,    6,    9,   22,   15,    3],\n",
              "       [  10,   12,   31,  851,    2,   45,    8,   13,   30,    8],\n",
              "       [   4,    3,   10,    2,  872,    6,    9,    5,   18,   53],\n",
              "       [  19,    5,    2,   57,    8,  721,   28,    3,   36,   13],\n",
              "       [  13,    3,    2,    2,   16,   25,  893,    0,    4,    0],\n",
              "       [   3,    7,   14,    6,   17,    2,    1,  887,   17,   74],\n",
              "       [  20,   14,    9,   43,   13,   46,   14,    7,  785,   23],\n",
              "       [  14,    9,    5,    7,   69,    0,    0,   22,   26,  857]])"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "confusion_matrix(y_test,results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkIboPhNuUZK"
      },
      "source": [
        "#The accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAQqya3XjXEo",
        "outputId": "2d3f72d8-6a5a-4134-a318-b3fb438eb075"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8808"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(y_test,results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAjzV6rIuZpr"
      },
      "source": [
        "#F1 score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sDh0GNwn2D2",
        "outputId": "390f8667-4a78-47db-d9a9-b45680333f2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8808"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f1_score(y_test,results,average='micro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkN_E6Xkuf8t"
      },
      "source": [
        "#Precision score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L8PiN8LuiPy",
        "outputId": "54e87fb1-88c8-4db4-b14d-4709454f6b7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8808"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "precision_score(y_test,results,average='micro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDR2ZyFfup7c"
      },
      "source": [
        "#Recall score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znr_G8cSuqFW",
        "outputId": "9f5be2d3-5ea6-466d-8b2c-7ec1bc316b28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8808"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "recall_score(y_test,results,average='micro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "-jsnTEcKu2hO",
        "outputId": "cd7978a1-cfeb-4f98-88ba-96cd4ed8b6a2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'testing_train=array_train.copy()    \\n    for h in range(thetas.shape[0]-1):\\n        testing_train = sigmoid(np.dot(testing_train,thetas[h].parameter_matrix))    \\n    testing_train = softmax(np.dot(testing_train,thetas[-1].parameter_matrix))\\n    error = np.dot(np.transpose(ground_truth), np.log2(testing_train)) + np.dot(np.transpose(1-ground_truth), np.log2(1-testing_train))\\n    error = -error\\n    error = np.sum(error)\\n    error = error/testing_train.shape[0]\\n    print(\"Cross entropy loss = \",error)'"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"testing_train=array_train.copy()\n",
        "    for h in range(thetas.shape[0]-1):\n",
        "        testing_train = sigmoid(np.dot(testing_train,thetas[h].parameter_matrix))\n",
        "    testing_train = softmax(np.dot(testing_train,thetas[-1].parameter_matrix))\n",
        "    error = np.dot(np.transpose(ground_truth), np.log2(testing_train)) + np.dot(np.transpose(1-ground_truth), np.log2(1-testing_train))\n",
        "    error = -error\n",
        "    error = np.sum(error)\n",
        "    error = error/testing_train.shape[0]\n",
        "    print(\"Cross entropy loss = \",error)\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGuXQ_3gLAIT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
