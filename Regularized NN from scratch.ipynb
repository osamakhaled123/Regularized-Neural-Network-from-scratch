{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DGO0EMfpcmY1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZZDdVnMayrs"
      },
      "source": [
        "#Loading Mnist dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Na87Tu3Ddt9S"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9D7-T6fdzdy",
        "outputId": "23d14a28-52ae-4786-cc0c-4dff3a5592e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (10000, 28, 28) \n",
            "\n",
            "(60000,) (10000,) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(x_train.shape,x_test.shape,\"\\n\")\n",
        "print(y_train.shape,y_test.shape,\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja-dmBFKXe0O"
      },
      "source": [
        "#Doing dimensionality reduction for Training set\n",
        "#and get the centroid of each sub-image, after splitting the image into 4X4 sub-images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JOpdEC6Fd250"
      },
      "outputs": [],
      "source": [
        "k = 0\n",
        "j=0\n",
        "x_axis = 0\n",
        "y_axis = 0\n",
        "\n",
        "array_train = np.zeros([60000,32])\n",
        "\n",
        "for t in range(x_train.shape[0]):\n",
        "    x_axis = 0\n",
        "    y_axis = 0\n",
        "    j = 0\n",
        "    k = 7\n",
        "    h = 7\n",
        "    i = 0\n",
        "    l = 0\n",
        "    img = np.zeros([16,7,7])\n",
        "\n",
        "    for i in range(16):\n",
        "        if i%4 == 0 and i != 0:\n",
        "            k += 7\n",
        "            h = 7\n",
        "        img[i] = x_train[t][k-7:k,h-7:h]\n",
        "        h += 7\n",
        "\n",
        "    for l in range(16):\n",
        "        for i in range(7):\n",
        "            for j in range(7):\n",
        "                x_axis += img[l][i][j]*i\n",
        "\n",
        "        if sum(sum(img[l])) == 0:\n",
        "            array_train[t][l] = 0\n",
        "\n",
        "        else:\n",
        "            array_train[t][l] = x_axis/sum(sum(img[l]))\n",
        "        x_axis = 0\n",
        "\n",
        "    ################################################\n",
        "\n",
        "    i = 0\n",
        "    j = 0\n",
        "    l = 0\n",
        "    y_axis = 0\n",
        "    for l in range(16):\n",
        "        for i in range(7):\n",
        "            for j in range(7):\n",
        "                y_axis = y_axis + img[l][j][i]*i\n",
        "\n",
        "        if sum(sum(img[l])) == 0:\n",
        "            array_train[t][l+16] = 0\n",
        "        else:\n",
        "            array_train[t][l+16] = y_axis/sum(sum(img[l]))\n",
        "        y_axis = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqVJHeo3bE_r"
      },
      "source": [
        "#Doing dimensionality reduction for Testing set\n",
        "#and get the centroid of each sub-image, after splitting the image into 4X4 sub-images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MTS5M0WTd4-b"
      },
      "outputs": [],
      "source": [
        "k = 0\n",
        "j=0\n",
        "x_axis = 0\n",
        "y_axis = 0\n",
        "array_test = np.zeros([10000,32])\n",
        "for t in range(x_test.shape[0]):\n",
        "    x_axis = 0\n",
        "    y_axis = 0\n",
        "    j = 0\n",
        "    i = 0\n",
        "    l = 0\n",
        "    img = np.zeros([16,7,7])\n",
        "    k = 7\n",
        "    h = 7\n",
        "\n",
        "    for i in range(16):\n",
        "        if i%4 == 0 and i != 0:\n",
        "            k += 7\n",
        "            h = 7\n",
        "        img[i] = x_test[t][k-7:k,h-7:h]\n",
        "        h += 7\n",
        "\n",
        "    for l in range(16):\n",
        "        for i in range(7):\n",
        "            for j in range(7):\n",
        "                x_axis += img[l][i][j]*i\n",
        "\n",
        "        if sum(sum(img[l])) == 0:\n",
        "            array_test[t][l] = 0\n",
        "\n",
        "        else:\n",
        "            array_test[t][l] = x_axis/sum(sum(img[l]))\n",
        "        x_axis = 0\n",
        "\n",
        "    ################################################\n",
        "\n",
        "    i = 0\n",
        "    j = 0\n",
        "    l = 0\n",
        "    y_axis = 0\n",
        "\n",
        "    for l in range(16):\n",
        "        for i in range(7):\n",
        "            for j in range(7):\n",
        "                y_axis = y_axis + img[l][j][i]*i\n",
        "\n",
        "        if sum(sum(img[l])) == 0:\n",
        "            array_test[t][l+16] = 0\n",
        "        else:\n",
        "            array_test[t][l+16] = y_axis/sum(sum(img[l]))\n",
        "        y_axis = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--kJBNC2gcaL"
      },
      "source": [
        "#Normalize the features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TOUhV4dEgbv_"
      },
      "outputs": [],
      "source": [
        "for i in range (array_train.shape[1]):\n",
        "    array_train[:,i] = (array_train[:,i] - np.mean(array_train[:,i])) / np.std(array_train[:,i])\n",
        "    array_test[:,i] = (array_test[:,i] - np.mean(array_test[:,i])) / np.std(array_test[:,i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjKKqzvPUtBJ"
      },
      "source": [
        "#Creating validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ikl3i1MXUXS6"
      },
      "outputs": [],
      "source": [
        "array_val = array_train[:6000,:]\n",
        "array_train = array_train[6000:,:]\n",
        "y_val = y_train[:6000]\n",
        "y_train = y_train[6000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tiWm1suU21z",
        "outputId": "15ff8384-f448-4416-e720-00d0493ac33c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((6000, 32), (6000,), (54000, 32), (54000,))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "array_val.shape, y_val.shape, array_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYWdeDbrbku6"
      },
      "source": [
        "#implementing sigmoid activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4eTXyu1AhyIH"
      },
      "outputs": [],
      "source": [
        "def sigmoid(array):\n",
        "    return 1/(1+np.exp(-array.copy(),dtype = np.float128))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGlqgHSffU1i"
      },
      "source": [
        "#Defining Softmax activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "B2gfekGzXpfs"
      },
      "outputs": [],
      "source": [
        "def softmax(array):\n",
        "    post_values = array.copy()\n",
        "    post_values = np.exp(post_values, dtype = np.float128)\n",
        "    sum_of_output_values = np.sum(post_values.copy(),axis=1)\n",
        "    for i in range(post_values.shape[1]):\n",
        "        post_values[:,i] /= sum_of_output_values\n",
        "    return post_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dwh9AhCYfJGh"
      },
      "source": [
        "#Defining function convert the integer number to a one-hot dimension with respect to the number of classes in the set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jQSXxp5Nh3oF"
      },
      "outputs": [],
      "source": [
        "def dimensions(y_train):\n",
        "    real_y = np.zeros(shape=(1,10))\n",
        "    real_y[:,y_train] = 1\n",
        "    return real_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McdTuWxne7nk"
      },
      "source": [
        "#Defining function computes the gradient **rate of change** of the loss values with respect to the weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hYpWvACih8N-"
      },
      "outputs": [],
      "source": [
        "def sigmoid_gradient(array):\n",
        "    return np.multiply(array,1-array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjHE4BH4ezxg"
      },
      "source": [
        "#Defining the loss function **Cross entropy** for classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bZs8mC8eKtWA"
      },
      "outputs": [],
      "source": [
        "def loss_function(x,y,parameters, reg_lambda):\n",
        "  h_theta = x\n",
        "  real_y = dimensions_10(y)\n",
        "  m = h_theta.shape[0]\n",
        "\n",
        "  for i in range(parameters.shape[0]-1):\n",
        "    h_theta = sigmoid(np.dot(h_theta,parameters[i].parameter_matrix))\n",
        "\n",
        "  h_theta = softmax(np.dot(h_theta,parameters[parameters.shape[0]-1].parameter_matrix))\n",
        "  cost = np.multiply(real_y.copy(),np.log(h_theta)) + np.multiply(1-real_y.copy(),np.log(1-h_theta))\n",
        "  regularized_cost = 0\n",
        "  for i in range(parameters.shape[0]):\n",
        "    regularized_cost += np.sum((np.square(np.array(parameters[i].parameter_matrix))))\n",
        "  regularized_cost *= (reg_lambda/(2*m))\n",
        "  cost = np.sum(cost)\n",
        "  cost /= m\n",
        "  cost = -cost\n",
        "  cost += regularized_cost\n",
        "  print(\"cost is \", end=\"\")\n",
        "  return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZUPUbYjedKA"
      },
      "source": [
        "#Defining the matrix class which is desired for initalize the weight matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SrB3ijS2h-YQ"
      },
      "outputs": [],
      "source": [
        "class matrix:\n",
        "    def __init__(self, current_layer, next_layer):\n",
        "        self.current_layer = current_layer\n",
        "        self.next_layer = next_layer\n",
        "        self.parameter_matrix = np.random.normal(size=(self.current_layer,self.next_layer))\n",
        "\n",
        "    def get_parameters(self):\n",
        "        return self.parameter_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUfUveereOm6"
      },
      "source": [
        "#Defining the Neural Network class to build the desired architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yFong6SpiASa"
      },
      "outputs": [],
      "source": [
        "class Neural_Network:\n",
        "    def __init__(self, layers, neurons):\n",
        "        self.layers = layers\n",
        "        self.neurons = neurons\n",
        "        self.parameter_matrices = np.empty(shape=(self.layers - 1,),dtype=matrix)\n",
        "        self.outputs = np.zeros(shape=(self.layers,),dtype=matrix)\n",
        "\n",
        "    def preparing(self, train):\n",
        "        data = train.copy()\n",
        "        self.parameter_matrices[0] = matrix(data.shape[1], self.neurons)\n",
        "        self.parameter_matrices[self.layers - 2] = matrix(self.neurons, 10)\n",
        "\n",
        "        for i in range(1,self.layers - 2):\n",
        "            self.parameter_matrices[i] = matrix(self.neurons, self.neurons)\n",
        "\n",
        "        return self.parameter_matrices\n",
        "\n",
        "    def feed_forward(self,train, parameters):\n",
        "        data = train.copy()\n",
        "        self.outputs[0] = matrix(data.shape[0], data.shape[1])\n",
        "        self.outputs[0].parameter_matrix = data.copy()\n",
        "\n",
        "        for i in range(1,self.layers-1):\n",
        "            data = sigmoid(np.dot(data,parameters[i-1].parameter_matrix))\n",
        "            self.outputs[i] = matrix(data.shape[0], data.shape[1])\n",
        "            self.outputs[i].parameter_matrix = data.copy()\n",
        "\n",
        "        data = softmax(np.dot(data,parameters[self.layers - 2].parameter_matrix))\n",
        "        self.outputs[self.layers - 1] = matrix(data.shape[0], data.shape[1])\n",
        "        self.outputs[self.layers - 1].parameter_matrix = data.copy()\n",
        "        return parameters.copy(), self.outputs\n",
        "\n",
        "    def back_propagation(self, outputs, parameters, target, learning_rate, reg_lambda):\n",
        "        actual_y = dimensions(target)\n",
        "        thetas = parameters.copy()\n",
        "        parameter = parameters.copy()\n",
        "        data = outputs.copy()\n",
        "\n",
        "        ########################################################\n",
        "        # for output layer only\n",
        "        error = np.multiply((actual_y - data[-1].parameter_matrix), sigmoid_gradient(data[-1].parameter_matrix))\n",
        "\n",
        "        thetas[-1].parameter_matrix += np.dot(np.transpose(data[-2].parameter_matrix), error) * learning_rate\n",
        "        # for output layer only\n",
        "        #########################################################\n",
        "\n",
        "        #########################################################\n",
        "        # for hidden layers...\n",
        "\n",
        "        for i in range(data.shape[0] - 2, 0 , -1):\n",
        "              error = np.dot(error,np.transpose(parameter[i].parameter_matrix))\n",
        "              error = np.multiply(error, sigmoid_gradient(data[i].parameter_matrix))\n",
        "              thetas[i-1].parameter_matrix += (np.dot(np.transpose(data[i-1].parameter_matrix), error) + (reg_lambda/data.shape[0])*data[i-1].parameter_matrix) * learning_rate\n",
        "\n",
        "        return thetas.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJOwCWUTeBYK"
      },
      "source": [
        "#Defining a function convert the target to on-hot dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Yhglkh-CI5ZZ"
      },
      "outputs": [],
      "source": [
        "def dimensions_10(y_train):\n",
        "    real_y = np.zeros(shape=(y_train.shape[0],np.unique(y_train).shape[0]))\n",
        "    for i in range(0,y_train.shape[0]):\n",
        "        real_y[i,y_train[i]] = 1\n",
        "    return real_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7vBH1dld9QP"
      },
      "source": [
        "#Defining the accuracy test function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MWXLqbWyn1Td"
      },
      "outputs": [],
      "source": [
        "def accuracy(data,target,parameters):\n",
        "    real_y = dimensions_10(target.copy())\n",
        "    results = data.copy()\n",
        "    thetas = parameters.copy()\n",
        "\n",
        "    for i in range(thetas.shape[0]-1):\n",
        "        results = sigmoid(np.dot(results, thetas[i].parameter_matrix))\n",
        "\n",
        "    results = softmax(np.dot(results, thetas[thetas.shape[0]-1].parameter_matrix))\n",
        "    hypothesis = np.zeros([results.shape[0],results.shape[1]])\n",
        "    count = 0\n",
        "\n",
        "    for m in range(0,results.shape[0]):\n",
        "        index_ = 0\n",
        "        biggest = results[m,index_]\n",
        "\n",
        "        for n in range(1,results.shape[1]):\n",
        "            if biggest < results[m,n]:\n",
        "                biggest = results[m,n]\n",
        "                index_ = n\n",
        "\n",
        "        hypothesis[m,index_] = 1\n",
        "\n",
        "    for l in range(0,real_y.shape[0]):\n",
        "        for v in range(0,real_y.shape[1]):\n",
        "            if real_y[l,v] == 1 and hypothesis[l,v] == 1:\n",
        "                count = count + 1\n",
        "\n",
        "\n",
        "    return(count/results.shape[0])*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cVEkBQ4dyrq"
      },
      "source": [
        "#initalize the Neural Network architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "d6-ao4Mgudzz"
      },
      "outputs": [],
      "source": [
        "N = Neural_Network(7,32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynPKqVE0dstv"
      },
      "source": [
        "#Defining fit Function for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-64wPwx7lSHC"
      },
      "outputs": [],
      "source": [
        "def fit(array_train, iterations, learning_rate, reg_lambda):\n",
        "    thetas = N.preparing(array_train.copy())\n",
        "    for iterations in range(0,iterations):\n",
        "        for i in range(0,array_train.shape[0]):\n",
        "            thetas, outputs = N.feed_forward(array_train[i:i+1,:],thetas)\n",
        "            thetas = N.back_propagation(outputs,thetas,y_train[i],learning_rate,reg_lambda)\n",
        "\n",
        "        print(\"epoch \",iterations+1,\": \\nLoss function on Training set\",loss_function(array_train,y_train,thetas, reg_lambda))\n",
        "        print(\"epoch \",iterations+1,\": \\nLoss function on Validation set\",loss_function(array_val,y_val,thetas, reg_lambda))\n",
        "        print(\"=========================================================\")\n",
        "    return thetas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAqGhPGn0jHZ",
        "outputId": "34db6182-b030-41d4-f905-edc65ba536ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost is epoch  1 : \n",
            "Loss function on Training set 2.1460886013008783732\n",
            "cost is epoch  1 : \n",
            "Loss function on Validation set 2.1096900097520314065\n",
            "=========================================================\n",
            "cost is epoch  2 : \n",
            "Loss function on Training set 1.5354730847034725134\n",
            "cost is epoch  2 : \n",
            "Loss function on Validation set 1.4902997556367340589\n",
            "=========================================================\n",
            "cost is epoch  3 : \n",
            "Loss function on Training set 1.2482825807849426428\n",
            "cost is epoch  3 : \n",
            "Loss function on Validation set 1.2132246403104544073\n",
            "=========================================================\n",
            "cost is epoch  4 : \n",
            "Loss function on Training set 1.1056036359740782928\n",
            "cost is epoch  4 : \n",
            "Loss function on Validation set 1.0763483650512458605\n",
            "=========================================================\n",
            "cost is epoch  5 : \n",
            "Loss function on Training set 1.016036709321464725\n",
            "cost is epoch  5 : \n",
            "Loss function on Validation set 0.98880387248501091553\n",
            "=========================================================\n",
            "cost is epoch  6 : \n",
            "Loss function on Training set 0.94877995057245291554\n",
            "cost is epoch  6 : \n",
            "Loss function on Validation set 0.92386918305088887424\n",
            "=========================================================\n",
            "cost is epoch  7 : \n",
            "Loss function on Training set 0.89404940393575072855\n",
            "cost is epoch  7 : \n",
            "Loss function on Validation set 0.87431233576457771065\n",
            "=========================================================\n",
            "cost is epoch  8 : \n",
            "Loss function on Training set 0.84773928764766788796\n",
            "cost is epoch  8 : \n",
            "Loss function on Validation set 0.83427064055529873593\n",
            "=========================================================\n",
            "cost is epoch  9 : \n",
            "Loss function on Training set 0.80921307828512569686\n",
            "cost is epoch  9 : \n",
            "Loss function on Validation set 0.80076014162583501375\n",
            "=========================================================\n",
            "cost is epoch  10 : \n",
            "Loss function on Training set 0.77570121042376978933\n",
            "cost is epoch  10 : \n",
            "Loss function on Validation set 0.77118580007874388036\n",
            "=========================================================\n",
            "cost is epoch  11 : \n",
            "Loss function on Training set 0.746223397469414633\n",
            "cost is epoch  11 : \n",
            "Loss function on Validation set 0.7454936769230039903\n",
            "=========================================================\n",
            "cost is epoch  12 : \n",
            "Loss function on Training set 0.72031200152664214063\n",
            "cost is epoch  12 : \n",
            "Loss function on Validation set 0.72268145379577168\n",
            "=========================================================\n",
            "cost is epoch  13 : \n",
            "Loss function on Training set 0.6969717410490441657\n",
            "cost is epoch  13 : \n",
            "Loss function on Validation set 0.7012104418180428478\n",
            "=========================================================\n",
            "cost is epoch  14 : \n",
            "Loss function on Training set 0.67593367373334557887\n",
            "cost is epoch  14 : \n",
            "Loss function on Validation set 0.68153532187882231503\n",
            "=========================================================\n",
            "cost is epoch  15 : \n",
            "Loss function on Training set 0.65767592730666019703\n",
            "cost is epoch  15 : \n",
            "Loss function on Validation set 0.66558444822261377514\n",
            "=========================================================\n",
            "cost is epoch  16 : \n",
            "Loss function on Training set 0.6424742177036789076\n",
            "cost is epoch  16 : \n",
            "Loss function on Validation set 0.65354066729226172614\n",
            "=========================================================\n",
            "cost is epoch  17 : \n",
            "Loss function on Training set 0.6292175271588009636\n",
            "cost is epoch  17 : \n",
            "Loss function on Validation set 0.6434283392807857683\n",
            "=========================================================\n",
            "cost is epoch  18 : \n",
            "Loss function on Training set 0.61732047612336488806\n",
            "cost is epoch  18 : \n",
            "Loss function on Validation set 0.63455222111232948085\n",
            "=========================================================\n",
            "cost is epoch  19 : \n",
            "Loss function on Training set 0.6063797828815744454\n",
            "cost is epoch  19 : \n",
            "Loss function on Validation set 0.62638975651940229707\n",
            "=========================================================\n",
            "cost is epoch  20 : \n",
            "Loss function on Training set 0.59631022230854803995\n",
            "cost is epoch  20 : \n",
            "Loss function on Validation set 0.6190909902830267055\n",
            "=========================================================\n"
          ]
        }
      ],
      "source": [
        "thetas = fit(array_train, 20, 0.01, 1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1fMXe3rfvx2"
      },
      "source": [
        "#printing the accuracy after training on the training set on **Training set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvNG-fhwoIqA",
        "outputId": "9b463468-e653-419b-8343-53ee57ab478a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of training set is:  89.63148148148147\n"
          ]
        }
      ],
      "source": [
        "print(\"accuracy of training set is: \", accuracy(array_train,y_train,thetas))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7OTIb_1qGx_"
      },
      "source": [
        "#printing the accuracy after training on the training set on **Test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNL7PBbWJc7U",
        "outputId": "3a4c9aa7-099c-4742-fbf5-df080c418a3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on test set:  89.32\n"
          ]
        }
      ],
      "source": [
        "print(\"accuracy on test set: \", accuracy(array_test,y_test,thetas))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-CSIEgFkcUO"
      },
      "source": [
        "#defining a function preparing for prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_F-obhLokbRW"
      },
      "outputs": [],
      "source": [
        "def preparing(result,parameters):\n",
        "    results = result.copy()\n",
        "    thetas = parameters.copy()\n",
        "    for i in range(thetas.shape[0]-1):\n",
        "        results = sigmoid(np.dot(results, thetas[i].parameter_matrix))\n",
        "\n",
        "    results = softmax(np.dot(results, thetas[thetas.shape[0]-1].parameter_matrix))\n",
        "\n",
        "    post_result = np.zeros(shape=(results.shape[0],))\n",
        "    for i in range(results.shape[0]):\n",
        "        index = np.argmax(results[i,:])\n",
        "        post_result[i]=index\n",
        "    return post_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXd1ce02gQGF"
      },
      "source": [
        "#See the Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "zOg6VPPBgQQC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Qt-NcLUeiP9A"
      },
      "outputs": [],
      "source": [
        "results = preparing(array_test,thetas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v6H4071uRf0"
      },
      "source": [
        "#Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy_YvN4rhznV",
        "outputId": "ff2a72f9-1b97-4587-b599-ec6ee61860ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 957,    0,    4,    5,    1,    6,    4,    3,    0,    0],\n",
              "       [   0, 1099,    5,    6,    5,    4,    3,    3,    7,    3],\n",
              "       [  11,    3,  942,   21,    6,    2,   10,   11,   24,    2],\n",
              "       [  12,    8,   31,  871,    2,   35,    5,   13,   26,    7],\n",
              "       [   0,    3,    8,    4,  893,    5,   11,    5,   12,   41],\n",
              "       [  13,    5,    1,   80,    8,  722,   17,    2,   36,    8],\n",
              "       [  12,    1,    9,    1,   10,   18,  898,    5,    4,    0],\n",
              "       [   2,    5,   18,    7,   21,    2,    4,  888,   10,   71],\n",
              "       [  14,    6,   14,   50,   23,   42,   14,    5,  779,   27],\n",
              "       [   7,    6,    2,    7,   65,    9,    0,   17,   13,  883]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "confusion_matrix(y_test,results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkIboPhNuUZK"
      },
      "source": [
        "#The accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAQqya3XjXEo",
        "outputId": "26299933-b1ba-4a4e-ee09-87260980771f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8932"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "accuracy_score(y_test,results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAjzV6rIuZpr"
      },
      "source": [
        "#F1 score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sDh0GNwn2D2",
        "outputId": "662a0ae8-fc1d-4def-b7a7-2385fab7e0f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8932"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "f1_score(y_test,results,average='micro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkN_E6Xkuf8t"
      },
      "source": [
        "#Precision score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L8PiN8LuiPy",
        "outputId": "de14cd91-649b-455d-d651-f202126666bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8932"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "precision_score(y_test,results,average='micro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDR2ZyFfup7c"
      },
      "source": [
        "#Recall score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znr_G8cSuqFW",
        "outputId": "af9a2e24-72b3-4e88-ca85-1c57770edf9e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8932"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "recall_score(y_test,results,average='micro')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}